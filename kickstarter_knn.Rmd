---
title: "Kickstarter Projects - kNN"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Classifying Kickstarter projects using k-nearest neighbors method
Import and clean data
```{r import and clean}
#import mostly cleaned dataset
df <- readRDS(gzcon(url("https://raw.githubusercontent.com/segevm/Classifying-Kickstarter-Projects/master/datasets/cleaned_data.rds")))
#drop main_category because the variable is already factored
df <- df[, -c(1)]
#convert duration to numeric
df$duration<-as.numeric(df$duration)
str(df)
```
Split data into training and testing sets (75%-25% split)
```{r split}
set.seed(123)
index<-sample(1:nrow(df), .75*nrow(df), replace=F)
train<-df[index,]
test<-df[-index,]
```

Fit kNN models with k = 1-10 to compare error rates and find best value for k
```{r knn}
#load FNN library
library(FNN)
#compute errors
overallerror<-data.frame(k=seq(1,10,1), error=rep(0,10))
for(i in 1:10){
  m<-knn(train, test, train$state, k=i)
  tb<-table(test$state, m)
  overallerror[i,2]<-(tb[1,2]+tb[2,1])/sum(tb)
}
overallerror
```

Plot
```{r plot}
plot(overallerror)
lines(overallerror)
```

It appears that 3 is a good value for k.
``` {r k=3}
#fit model
m<-knn(train, test, train$state, k=3)
```

Confusion matrix
```{r confusion matrix}
#confusion matrix
tb<-table(actual=test$state, predicted=m)
tb
```
Performance measures
```{r accuracy}
#compute accuracy
accuracy<-function(x){
  sum(diag(x)/(sum(rowSums(x))))
}
accuracy(tb)
```
ROC curves
```{r}
library(ROCR)
m<-as.numeric(m)
pred<-prediction(m, test$state)
perf<-performance(pred, measure="tpr", x.measure="fpr")
plot(perf, col=rainbow(10))
abline(0,1)
library(pROC)
plot.roc(test$state, m, col=rainbow(10))
```
AUC
```{r}
auc<-performance(pred, "auc")
auc@y.values
```

